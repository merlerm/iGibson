{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65468f7e-20d8-498f-b28f-65e312af77e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _   _____  _  _\n",
      "(_) / ____|(_)| |\n",
      " _ | |  __  _ | |__   ___   ___   _ __\n",
      "| || | |_ || || '_ \\ / __| / _ \\ | '_ \\\n",
      "| || |__| || || |_) |\\__ \\| (_) || | | |\n",
      "|_| \\_____||_||_.__/ |___/ \\___/ |_| |_|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract from https://github.com/StanfordVL/behavior/blob/main/behavior/benchmark/behavior_benchmark.py\n",
    "import os\n",
    "import bddl\n",
    "import json\n",
    "import behavior\n",
    "import pyquaternion  \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from igibson.utils.utils import parse_config\n",
    "from igibson.envs.igibson_env import iGibsonEnv\n",
    "from igibson import object_states\n",
    "\n",
    "from utils import get_env_config, render_robot_pov, render_360, render_robot_orientation_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be27947-2fd1-4348-9c2c-a46b3d2d6c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c26f26d-e891-4f6e-b4e2-68cfffb7ac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01b1fde4-7ed7-44fe-af19-1532739cf197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering settings is None - going with the default settings!\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frameb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "left_hand_shoulderb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frameb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "right_hand_shoulderb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frameb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "neckb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frameb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "eyesAngle: 0 - View direction: [ 9.99998383e-01 -3.27519636e-05 -1.79802528e-03]\n",
      "Angle: 45 - View direction: [ 0.70712664  0.7070826  -0.00247246]\n",
      "Angle: 90 - View direction: [ 2.96979133e-05  9.99998557e-01 -1.69855847e-03]\n",
      "Angle: 135 - View direction: [-7.07084638e-01  7.07128920e-01  7.03336537e-05]\n",
      "Angle: 180 - View direction: [-9.99998383e-01  3.27519636e-05  1.79802528e-03]\n",
      "Angle: 225 - View direction: [-0.70712664 -0.7070826   0.00247246]\n",
      "Angle: 270 - View direction: [-2.96979133e-05 -9.99998557e-01  1.69855847e-03]\n",
      "Angle: 315 - View direction: [ 7.07084638e-01 -7.07128920e-01 -7.03336537e-05]\n",
      "Angle: 360 - View direction: [ 9.99998383e-01 -3.27519636e-05 -1.79802528e-03]\n"
     ]
    }
   ],
   "source": [
    "PHYSICS_STEPS = 50\n",
    "\n",
    "scene_id =  \"Benevolence_1_int\"\n",
    "task = \"cleaning_out_drawers\"\n",
    "env_config = get_env_config()\n",
    "env_config[\"scene_id\"] = scene_id\n",
    "env_config[\"task\"] = task\n",
    "env_config[\"task_id\"] = 0\n",
    "env_config[\"instance_id\"] = 0\n",
    "\n",
    "# Keep this in memory as it's removed from the config when we init iGibson - so we need to reload it\n",
    "robot_name = env_config[\"robot\"][\"name\"]\n",
    "\n",
    "env = iGibsonEnv(\n",
    "        config_file=env_config,\n",
    "        mode=\"headless\",\n",
    "        action_timestep=1.0 / 30.0,\n",
    "        physics_timestep=1.0 / 120.0,\n",
    "    )\n",
    "\n",
    "render_robot_pov(env, env_config, step='initial', show=False, save=False)\n",
    "\n",
    "# Run some steps to let physics settle.\n",
    "s = env.simulator\n",
    "for _ in range(PHYSICS_STEPS):\n",
    "    s.step()\n",
    "    \n",
    "render_robot_pov(env, env_config, step='initial', show=False, save=True)\n",
    "render_360(env, show=False, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24522838-cef2-4ddd-a9ef-b695ab707198",
   "metadata": {},
   "source": [
    "## Navigate to obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "434dae13-ac14-4ac6-86c8-a803c037785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_properties(obj):\n",
    "    \"\"\"\n",
    "    Requires from igibson import object_states\n",
    "    \"\"\"\n",
    "    logic_state = {}\n",
    "    state_keys = ['InFOVOfRobot', 'InSameRoomAsRobot', 'InReachOfRobot']\n",
    "\n",
    "    logic_state = {key: obj.states[getattr(object_states, key)].get_value() for key in state_keys}\n",
    "\n",
    "    print(logic_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "33207599-ac1e-41c6-86f2-2c4973154b43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The StarterSemanticActionPrimitive is a work-in-progress and is only provided as an example. It currently only works with BehaviorRobot with its JointControllers set to absolute mode. See provided behavior_robot_mp_behavior_task.yaml config file for an example. See examples/action_primitives for runnable examples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "                                                    \n",
      "Attempt: 0\n",
      "[ 0.5980776  -0.59241134  0.44275743] [0.51700539 0.03961183 0.2340856 ]\n",
      "Yaw:  0.20236986698387183 11.59493927879972\n",
      "New yaw:  -1.8968451162074984 -108.68121954869184\n",
      "                                                        Start collision test.\n",
      "                                                        Body has collision with objects %s [28]\n",
      "                                                        End collision test.\n",
      "                                                      Candidate position failed collision test.\n",
      "                                                    \n",
      "Attempt: 1\n",
      "[ 0.5980776  -0.59241134  0.44275743] [ 0.74891984 -1.12769443  0.21944658]\n",
      "Yaw:  1.9077054044516721 109.30346822937855\n",
      "New yaw:  1.736724108756726 99.50696161037978\n",
      "                                                        Start collision test.\n",
      "                                                        Body has collision with objects %s [0]\n",
      "                                                        Left hand has collision with objects %s [24]\n",
      "                                                        Right hand has collision with objects %s [24]\n",
      "                                                        End collision test.\n",
      "                                                      Candidate position failed collision test.\n",
      "                                                    \n",
      "Attempt: 2\n",
      "[ 0.5980776  -0.59241134  0.44275743] [ 0.58715589 -1.12769443  0.26282423]\n",
      "Yaw:  -0.9064544224555413 -51.93601272767107\n",
      "New yaw:  1.7970649654834543 102.96423803302488\n",
      "                                                    Candidate position is in the wrong room.\n",
      "                                                    \n",
      "Attempt: 3\n",
      "[ 0.5980776  -0.59241134  0.44275743] [0.63265193 0.03961183 0.25627234]\n",
      "Yaw:  -1.960142070358058 -112.307867877552\n",
      "New yaw:  -1.2477521860634473 -71.49093413965775\n",
      "                                                        Start collision test.\n",
      "                                                        Body has collision with objects %s [24]\n",
      "                                                        Left hand has collision with objects %s [24]\n",
      "                                                        Right hand has collision with objects %s [24]\n",
      "                                                        End collision test.\n",
      "                                                      Candidate position failed collision test.\n",
      "                                                    \n",
      "Attempt: 4\n",
      "[ 0.5980776  -0.59241134  0.44275743] [0.44794814 0.03961183 0.83379644]\n",
      "Yaw:  -1.3428083923321896 -76.93725357538169\n",
      "New yaw:  -1.3311398465636013 -76.26869514978632\n",
      "                                                        Start collision test.\n",
      "                                                        Body has collision with objects %s [24]\n",
      "                                                        Left hand has collision with objects %s [24]\n",
      "                                                        Right hand has collision with objects %s [24]\n",
      "                                                        End collision test.\n",
      "                                                      Candidate position failed collision test.\n",
      "                                                    \n",
      "Attempt: 5\n",
      "[ 0.5980776  -0.59241134  0.44275743] [ 0.25006457 -0.67480692  0.68864763]\n",
      "Yaw:  -1.609649637329287 -92.22613071373175\n",
      "New yaw:  1.0943562528682986 62.701994573105\n",
      "                                                    Candidate position is in the wrong room.\n",
      "                                                    \n",
      "Attempt: 6\n",
      "[ 0.5980776  -0.59241134  0.44275743] [0.61500703 0.03961183 0.87385209]\n",
      "Yaw:  -0.7750352223499424 -44.40624721463503\n",
      "New yaw:  3.0959462317698168 177.38465267984148\n",
      "                                                        Start collision test.\n",
      "                                                        Body has collision with objects %s [0]\n",
      "                                                        End collision test.\n",
      "                                                      Candidate position failed collision test.\n",
      "                                                    \n",
      "Attempt: 7\n",
      "[ 0.5980776  -0.59241134  0.44275743] [ 0.25006457 -0.71974802  0.85555888]\n",
      "Yaw:  -3.0980237308026157 -177.50368460636338\n",
      "New yaw:  0.24257934978407222 13.898772939655077\n",
      "                                                        Start collision test.\n",
      "                                                        Body has collision with objects %s [0]\n",
      "                                                        Right hand has collision with objects %s [0]\n",
      "                                                        End collision test.\n",
      "                                                      Candidate position failed collision test.\n",
      "                                                    \n",
      "Attempt: 8\n",
      "[ 0.5980776  -0.59241134  0.44275743] [0.67012259 0.03961183 0.63445992]\n",
      "Yaw:  2.5051596809926515 143.5350767272186\n",
      "New yaw:  -1.148349205807167 -65.79556289995064\n",
      "                                                        Start collision test.\n",
      "                                                        End collision test.\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n"
     ]
    }
   ],
   "source": [
    "# Go to cabinet 1\n",
    "from igibson.action_primitives.starter_semantic_action_primitives import StarterSemanticActionPrimitives\n",
    "\n",
    "scene = env.scene\n",
    "s = env.simulator\n",
    "robot = env.robots[0]\n",
    "controller = StarterSemanticActionPrimitives(None, scene, robot)\n",
    "#obj = env.task.object_scope['sink.n.01_1']\n",
    "#obj = env.task.object_scope['cabinet.n.01_1']\n",
    "obj = env.task.object_scope['cabinet.n.01_2']\n",
    "\n",
    "# Show before\n",
    "render_robot_pov(env, env_config, step='before', show=True, save=True)\n",
    "#print(\"Joints velocities before teleport: \", robot._joint_state['unnormalized']['velocity'])\n",
    "print_properties(obj)\n",
    "controller.teleport_near_obj(obj, env)\n",
    "#robot_pos, original_orientation = robot.get_position_orientation() # does it work reliably?\n",
    "#print(\"Joints velocities after teleport: \", robot._joint_state['unnormalized']['velocity'])\n",
    "\n",
    "# Run some steps to let physics settle.\n",
    "s = env.simulator\n",
    "for _ in range(PHYSICS_STEPS):\n",
    "    print_properties(obj)\n",
    "    #print(f\"Joints velocities after step {_}: \", robot._joint_state['unnormalized']['velocity']) # doesn't help\n",
    "    #robot_pos, robot_orientation = robot.get_position_orientation()\n",
    "    #print(\"robot_pos: \", robot_pos)\n",
    "    #print(\"robot_orientation: \", robot_orientation)\n",
    "    render_robot_pov(env, env_config, step=f'{_}', show=False, save=True)\n",
    "    #render_robot_orientation_view(env, original_orientation, env_config, step=f'{_}', show=False, save=True)\n",
    "    s.step()\n",
    "    \n",
    "# Show after -> works only if we save the original orientation \n",
    "# Is this enough, or do we want to try and force it on the robot?\n",
    "#render_robot_orientation_view(env, original_orientation, env_config, step='after_original_view', show=True, save=True)\n",
    "render_robot_pov(env, env_config, step='after', show=True, save=True)\n",
    "print_properties(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed80728d-e9c9-4b26-a2a3-aeca1098bc51",
   "metadata": {},
   "source": [
    "## Let's just put a blue dot over the right object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1f8700bd-7eda-4772-b2d2-3b9c80cfac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obj_pos, obj_orn = obj.get_position_orientation()\n",
    "#obj_pos\n",
    "\n",
    "# Let's get multiple objects over the surface \n",
    "positions_on_obj_surface = []\n",
    "for i in range(1000):\n",
    "    pos_on_obj_surface = controller._sample_position_on_aabb_face(obj)\n",
    "    positions_on_obj_surface.append(pos_on_obj_surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "27247402-abce-4483-ba0c-89565b39d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set camera to position that will be used for the bounding box! Otherwise we're going to have a mismatch\n",
    "robot_pos, robot_orientation = robot.get_position_orientation()\n",
    "\n",
    "# Convert quaternion to rotation matrix - takes w,x,y,z in input, but robot orientation is given as x,y,z,w !!!\n",
    "q = pyquaternion.Quaternion(x=robot_orientation[0], \n",
    "                            y=robot_orientation[1], \n",
    "                            z=robot_orientation[2], \n",
    "                            w=robot_orientation[3])\n",
    "\n",
    "forward_downward_direction = q.rotate(np.array([1, 0, -0.25]))  # Default forward vector (x-axis)\n",
    "up_direction = q.rotate(np.array([0, 0, 1]))  # Default up vector (z-axis)\n",
    "\n",
    "# Set the camera at the robot's head level (optional: raise it slightly)\n",
    "camera_pose = robot_pos + q.rotate(np.array([0.1, 0.1, 1])) # Slightly above the robot's center\n",
    "\n",
    "# Set the camera in the renderer\n",
    "s.renderer.set_camera(camera_pose, camera_pose + forward_downward_direction, up_direction)\n",
    "s.renderer.set_fov(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3554809e-d123-44bc-8e6e-99435362e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming is in world coordinates - move point to camera frame coordinates\n",
    "objects_pos_camera_frame = [s.renderer.transform_point(obj_pos) for obj_pos in positions_on_obj_surface]\n",
    "objects_pos_camera_frame = np.stack(objects_pos_camera_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d7238960-7712-4863-ae1a-c99425e9ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project to uv plane\n",
    "K = s.renderer.get_intrinsics()\n",
    "uvw_column_vectors = np.dot(K, objects_pos_camera_frame.T)\n",
    "uv_prime_column_vectors = uvw_column_vectors[:2]/uvw_column_vectors[2:]\n",
    "uv_prime = uv_prime_column_vectors.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "df502ef6-4752-4130-8608-da656bb4449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "frame = s.renderer.render(modes=(\"rgb\"))[0]\n",
    "rgb_image = (frame[..., :3] * 255).astype(np.uint8) \n",
    "\n",
    "# Save using PIL\n",
    "image = Image.fromarray(rgb_image)\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "# Draw point\n",
    "for (u,v) in uv_prime:\n",
    "    r = 3  # Radius of the circle\n",
    "    draw.ellipse([(u - r, v - r), (u + r, v + r)], fill=\"blue\", outline=\"blue\")\n",
    "    \n",
    "image.show()\n",
    "image.save(\"images/tmp.jpg\", \"JPEG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce186ba2-db43-4050-90e3-ee1a4d2748c1",
   "metadata": {},
   "source": [
    "### Sanity checks\n",
    "\n",
    "What happens to the camera position in the world frame when I transform the point in the camera frame? Does it go to (0,0,0)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc747e6-cc0f-48a3-94ac-52096c73dd19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "661f6e76-b2a7-47db-9031-6baf2fa5c3e2",
   "metadata": {},
   "source": [
    "## Render bounding boxes around task's objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd56d43-f4ed-4050-9bc0-9f40cfdb4d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ca710dc-9421-4ebc-a52e-dacb1f3e3504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://github.com/StanfordVL/iGibson/blob/master/igibson/examples/objects/draw_bounding_box.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de784928-2f80-4a81-882a-956eb695e28b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object is visible: True\n",
      "bbox_center_in_world [1.05994913 0.145      0.44992262]\n",
      "bbox_orn_in_world [ 0.          0.         -0.70710663  0.70710693]\n",
      "bbox_extent_in_desired_frame [0.76950285 0.57980715 0.89063036]\n",
      "bbox_center_in_desired_frame [ 0.01173455 -0.00954846  0.04806644]\n"
     ]
    }
   ],
   "source": [
    "import trimesh\n",
    "import itertools\n",
    "from igibson.utils import utils as ig_utils\n",
    "import pybullet as p \n",
    "\n",
    "visible = obj.states[object_states.InFOVOfRobot].get_value() \n",
    "print(f\"Object is visible: {visible}\")\n",
    "\n",
    "#bbox_center, bbox_orn, bbox_bf_extent, bbox_wf_extent = obj.get_base_aligned_bounding_box(visual=True)\n",
    "bbox_center_in_world, bbox_orn_in_world, bbox_extent_in_desired_frame, bbox_center_in_desired_frame  = obj.get_base_aligned_bounding_box(visual=True) \n",
    "print('bbox_center_in_world', bbox_center_in_world)\n",
    "print('bbox_orn_in_world', bbox_orn_in_world)\n",
    "# What is the desired frame??? -> Center of Mass frame\n",
    "print('bbox_extent_in_desired_frame', bbox_extent_in_desired_frame)\n",
    "print('bbox_center_in_desired_frame', bbox_center_in_desired_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e4de26-404a-45af-87b6-75ed6e01a1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b11b2837-1f2d-4ade-9428-aee105ebdcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbox_frame_vertex_positions \n",
      " [[ 0.39648598  0.28035511  0.49338162]\n",
      " [ 0.39648598  0.28035511 -0.39724874]\n",
      " [ 0.39648598 -0.29945204  0.49338162]\n",
      " [ 0.39648598 -0.29945204 -0.39724874]\n",
      " [-0.37301688  0.28035511  0.49338162]\n",
      " [-0.37301688  0.28035511 -0.39724874]\n",
      " [-0.37301688 -0.29945204  0.49338162]\n",
      " [-0.37301688 -0.29945204 -0.39724874]]\n",
      "bbox_transform.shape:  (4, 4)\n",
      "bbox_transform \n",
      " [[ 4.21468510e-07  1.00000000e+00  0.00000000e+00  1.05994913e+00]\n",
      " [-1.00000000e+00  4.21468510e-07 -0.00000000e+00  1.45000002e-01]\n",
      " [-0.00000000e+00  0.00000000e+00  1.00000000e+00  4.49922621e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "world_frame_vertex_positions: \n",
      " [[ 1.34030441 -0.25148586  0.94330424]\n",
      " [ 1.34030441 -0.25148586  0.05267388]\n",
      " [ 0.76049726 -0.2514861   0.94330424]\n",
      " [ 0.76049726 -0.2514861   0.05267388]\n",
      " [ 1.34030409  0.518017    0.94330424]\n",
      " [ 1.34030409  0.518017    0.05267388]\n",
      " [ 0.76049693  0.51801675  0.94330424]\n",
      " [ 0.76049693  0.51801675  0.05267388]]\n",
      "world_frame_vertex_positions (manual): \n",
      " [[ 1.34030441 -0.25148586  0.94330424]\n",
      " [ 1.34030441 -0.25148586  0.05267388]\n",
      " [ 0.76049726 -0.2514861   0.94330424]\n",
      " [ 0.76049726 -0.2514861   0.05267388]\n",
      " [ 1.34030409  0.518017    0.94330424]\n",
      " [ 1.34030409  0.518017    0.05267388]\n",
      " [ 0.76049693  0.51801675  0.94330424]\n",
      " [ 0.76049693  0.51801675  0.05267388]]\n"
     ]
    }
   ],
   "source": [
    "# Define the 3D bounding box corners - 8 corners in 3D space - does this make it a cube? I think not, because the 3 dimensions have different multipliers\n",
    "bbox_frame_vertex_positions = np.array(list(itertools.product((1, -1), repeat=3))) * (\n",
    "    bbox_extent_in_desired_frame / 2\n",
    ") + bbox_center_in_desired_frame\n",
    "print('bbox_frame_vertex_positions \\n', bbox_frame_vertex_positions)\n",
    "\n",
    "# Trasform corners to world coordinates by applying the translation + rotation - how can I know that it's correct?\n",
    "\n",
    "# 1. Convert position and quaternion to transformation matrix\n",
    "bbox_transform = ig_utils.quat_pos_to_mat(bbox_center_in_world, bbox_orn_in_world) \n",
    "print(\"bbox_transform.shape: \", bbox_transform.shape)\n",
    "print('bbox_transform \\n', bbox_transform)\n",
    "\n",
    "world_frame_vertex_positions = trimesh.transformations.transform_points(\n",
    "    bbox_frame_vertex_positions, bbox_transform\n",
    ")\n",
    "print(\"world_frame_vertex_positions: \\n\", world_frame_vertex_positions)# Compute the rotation matrix using the quaternion\n",
    "\n",
    "# Here, we assume bbox_orn_in_world is in (x, y, z, w) order. - how do we know if it's the right order??\n",
    "q = pyquaternion.Quaternion(x=bbox_orn_in_world[0], \n",
    "                            y=bbox_orn_in_world[1], \n",
    "                            z=bbox_orn_in_world[2], \n",
    "                            w=bbox_orn_in_world[3])\n",
    "rotation_matrix = q.rotation_matrix  # This is a 3x3 matrix\n",
    "\n",
    "# Manually apply rotation and translation:\n",
    "world_frame_vertex_positions = (bbox_frame_vertex_positions @ rotation_matrix.T) + bbox_center_in_world\n",
    "print(\"world_frame_vertex_positions (manual): \\n\", world_frame_vertex_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a875b29f-8d10-4557-a884-bea21d9cc978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set camera to position that will be used for the bounding box! Otherwise we're going to have a mismatch\n",
    "robot_pos, robot_orientation = robot.get_position_orientation()\n",
    "\n",
    "# Convert quaternion to rotation matrix - takes w,x,y,z in input, but robot orientation is given as x,y,z,w !!!\n",
    "q = pyquaternion.Quaternion(x=robot_orientation[0], \n",
    "                            y=robot_orientation[1], \n",
    "                            z=robot_orientation[2], \n",
    "                            w=robot_orientation[3])\n",
    "\n",
    "forward_downward_direction = q.rotate(np.array([1, 0, -0.25]))  # Default forward vector (x-axis)\n",
    "up_direction = q.rotate(np.array([0, 0, 1]))  # Default up vector (z-axis)\n",
    "\n",
    "# Set the camera at the robot's head level (optional: raise it slightly)\n",
    "camera_pose = robot_pos + q.rotate(np.array([0.1, 0.1, 1])) # Slightly above the robot's center\n",
    "\n",
    "# Set the camera in the renderer\n",
    "s.renderer.set_camera(camera_pose, camera_pose + forward_downward_direction, up_direction)\n",
    "s.renderer.set_fov(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "678efc1e-02c1-43b3-a161-06e1a703b984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera_frame_vertex_positions: \n",
      " [[-1.22247772 -0.02824842 -2.38847702]\n",
      " [-1.22247772 -0.89228677 -2.60448657]\n",
      " [-0.88693993 -0.1429323  -1.92974142]\n",
      " [-0.88693993 -1.00697065 -2.14575097]\n",
      " [-1.85003528 -0.13625334 -1.95645727]\n",
      " [-1.85003528 -1.00029169 -2.17246682]\n",
      " [-1.5144975  -0.25093721 -1.49772167]\n",
      " [-1.5144975  -1.11497556 -1.71373122]]\n"
     ]
    }
   ],
   "source": [
    "# Does setting the camera changes the outcome of the following function? Yes it does\n",
    "camera_frame_vertex_positions = np.array([s.renderer.transform_point(v) for v in world_frame_vertex_positions]) # still 3d!!\n",
    "print(\"camera_frame_vertex_positions: \\n\", camera_frame_vertex_positions)\n",
    "#bbox_orn_camera = s.renderer.transform_pose(bbox_orn) - might be broken\n",
    "#print(bbox_orn_camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "746e6971-1a4a-479d-9117-2ef4b8e56fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K \n",
      " [[540.   0. 540.]\n",
      " [  0. 540. 540.]\n",
      " [  0.   0.   1.]]\n"
     ]
    }
   ],
   "source": [
    "K = s.renderer.get_intrinsics()\n",
    "print('K \\n', K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "94b2f0dd-1c97-405a-82d9-36d09dba0d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.94991556e+03 -1.30503174e+03 -2.38847702e+00]\n",
      " [-2.06656071e+03 -1.88825760e+03 -2.60448657e+00]\n",
      " [-1.52100793e+03 -1.11924381e+03 -1.92974142e+00]\n",
      " [-1.63765309e+03 -1.70246967e+03 -2.14575097e+00]\n",
      " [-2.05550598e+03 -1.13006373e+03 -1.95645727e+00]\n",
      " [-2.17215113e+03 -1.71328959e+03 -2.17246682e+00]\n",
      " [-1.62659835e+03 -9.44275797e+02 -1.49772167e+00]\n",
      " [-1.74324351e+03 -1.52750166e+03 -1.71373122e+00]]\n",
      "(8, 2)\n",
      "[[ 816.38447482  546.386559  ]\n",
      " [ 793.46184351  725.00185945]\n",
      " [ 788.19261262  579.99677909]\n",
      " [ 763.20743274  793.41437932]\n",
      " [1050.62656321  577.60716076]\n",
      " [ 999.85468864  788.637865  ]\n",
      " [1086.04848543  630.47481755]\n",
      " [1017.22107254  891.33094189]]\n"
     ]
    }
   ],
   "source": [
    "uvw_column_vectors = np.dot(K, camera_frame_vertex_positions.T)\n",
    "print(uvw_column_vectors.T)\n",
    "uv_prime_column_vectors = uvw_column_vectors[:2]/uvw_column_vectors[2:]\n",
    "uv_prime = uv_prime_column_vectors.T\n",
    "print(uv_prime.shape)\n",
    "print(uv_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c9bf79a7-0285-425d-92a9-d45af73a8be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1086.0484854287465 763.2074327359236 891.3309418897617 546.386559002358\n"
     ]
    }
   ],
   "source": [
    "u_max, v_max = uv_prime.max(axis=0)\n",
    "u_min, v_min = uv_prime.min(axis=0)\n",
    "\n",
    "print(u_max, u_min, v_max, v_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0c28724b-ef8b-413f-bcf5-d75fc5788841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "frame = s.renderer.render(modes=(\"rgb\"))[0]\n",
    "rgb_image = (frame[..., :3] * 255).astype(np.uint8) \n",
    "\n",
    "# Save using PIL\n",
    "image = Image.fromarray(rgb_image)\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "# Draw all 8 points\n",
    "for i in range(8):\n",
    "    u, v = uv_prime[i]\n",
    "    r = 5  # Radius of the circle\n",
    "    draw.ellipse([(u - r, v - r), (u + r, v + r)], fill=\"blue\", outline=\"blue\")\n",
    "    \n",
    "# Draw the bounding box. You can adjust the outline color and width as needed.\n",
    "draw.rectangle([(u_min, v_min), (u_max, v_max)], outline=\"red\", width=3)\n",
    "#draw.rectangle([(v_min, u_min), (v_max, u_max)], outline=\"red\", width=3)\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93222ecc-e15b-48da-a140-c77933a289e5",
   "metadata": {},
   "source": [
    "## Alternative computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bf892a2f-4d91-42ad-b198-656da4014e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.56890081, -0.41037705, -0.85996199,  1.        ],\n",
       "       [ 0.56890081, -1.2744154 , -1.07597154,  1.        ],\n",
       "       [ 0.4019786 , -0.54504728, -0.32128098,  1.        ],\n",
       "       [ 0.4019786 , -1.40908563, -0.53729053,  1.        ],\n",
       "       [-0.16802338, -0.35664713, -1.07488173,  1.        ],\n",
       "       [-0.16802338, -1.22068548, -1.29089128,  1.        ],\n",
       "       [-0.33494558, -0.49131735, -0.53620072,  1.        ],\n",
       "       [-0.33494558, -1.3553557 , -0.75221027,  1.        ]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert world points to homogeneous coordinates:\n",
    "world_points_h = np.hstack((world_frame_vertex_positions, np.ones((world_frame_vertex_positions.shape[0], 1))))\n",
    "\n",
    "# Compute camera frame coordinates:\n",
    "camera_points_h = (s.renderer.V @ world_points_h.T).T  # Each row is a homogeneous point in camera space\n",
    "camera_points_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "455401de-16d4-4e7f-a187-70562d4ff747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize if necessary (usually w should be 1 after applying V)\n",
    "camera_points = camera_points_h[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "98736b38-c960-46f5-ba91-7f44b56d75d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 897.23257249,  282.30999886],\n",
       "       [ 825.51539104,  -99.59341759],\n",
       "       [1215.63428533, -376.10007114],\n",
       "       [ 944.00571884, -876.19142236],\n",
       "       [ 455.58827373,  360.82731256],\n",
       "       [ 469.71319584,   29.3681853 ],\n",
       "       [ 202.68114729,   45.20138909],\n",
       "       [ 299.54780258, -432.988686  ]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract intrinsic parameters from K:\n",
    "K = s.renderer.get_intrinsics()\n",
    "fu = K[0, 0]\n",
    "fv = K[1, 1]\n",
    "u0 = K[0, 2]\n",
    "v0 = K[1, 2]\n",
    "\n",
    "uv_points = []\n",
    "for pt in camera_points:\n",
    "    x_cam, y_cam, z_cam = pt\n",
    "    # Check that the point is in front of the camera (z_cam should be negative)\n",
    "    if z_cam >= 0:\n",
    "        uv_points.append([np.nan, np.nan])  # or handle points behind the camera appropriately\n",
    "    else:\n",
    "        u = fu * (x_cam / -z_cam) + u0\n",
    "        v = fv * (y_cam / -z_cam) + v0\n",
    "        uv_points.append([u, v])\n",
    "uv_prime = np.array(uv_points)\n",
    "uv_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f21d39a6-2b94-40a0-b329-7f29a3aff8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1215.6342853328802 202.68114729144668 360.82731256346347 -876.1914223601816\n"
     ]
    }
   ],
   "source": [
    "u_max, v_max = uv_prime.max(axis=0)\n",
    "u_min, v_min = uv_prime.min(axis=0)\n",
    "\n",
    "print(u_max, u_min, v_max, v_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eecf9408-9b98-49e9-aba6-090f8ef5f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "frame = s.renderer.render(modes=(\"rgb\"))[0]\n",
    "rgb_image = (frame[..., :3] * 255).astype(np.uint8) \n",
    "\n",
    "# Save using PIL\n",
    "image = Image.fromarray(rgb_image)\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "# Draw all 8 points\n",
    "for i in range(8):\n",
    "    u, v = uv_prime[i]\n",
    "    r = 5  # Radius of the circle\n",
    "    draw.ellipse([(u - r, v - r), (u + r, v + r)], fill=\"blue\", outline=\"blue\")\n",
    "    \n",
    "# Draw the bounding box. You can adjust the outline color and width as needed.\n",
    "draw.rectangle([(u_min, v_min), (u_max, v_max)], outline=\"red\", width=3)\n",
    "#draw.rectangle([(v_min, u_min), (v_max, u_max)], outline=\"red\", width=3)\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a212e13f-3272-42b0-b205-24b532f10bba",
   "metadata": {},
   "source": [
    "## Get list of objects and compute some predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6e02326-364e-4009-82b6-fca184aee9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object_names_list:  ['bowl.n.01_1', 'bowl.n.01_2', 'cabinet.n.01_1', 'cabinet.n.01_2', 'spoon.n.01_1', 'spoon.n.01_2', 'piece_of_cloth.n.01_1', 'sink.n.01_1', 'floor.n.01_1', 'agent.n.01_1']\n"
     ]
    }
   ],
   "source": [
    "object_names_list = list(env.task.object_scope.keys())\n",
    "print(\"object_names_list: \", object_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c06a235-0648-4adc-9a09-1ccf7d4b98d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object_names_list:  ['bowl.n.01_1', 'bowl.n.01_2', 'cabinet.n.01_1', 'cabinet.n.01_2', 'spoon.n.01_1', 'spoon.n.01_2', 'piece_of_cloth.n.01_1', 'sink.n.01_1', 'floor.n.01_1', 'agent.n.01_1']\n",
      "filtered_object_names_list:  ['bowl.n.01_1', 'bowl.n.01_2', 'cabinet.n.01_1', 'cabinet.n.01_2', 'spoon.n.01_1', 'spoon.n.01_2', 'piece_of_cloth.n.01_1', 'sink.n.01_1']\n",
      "Name: bowl.n.01_1\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': False, 'InReachOfRobot': True}\n",
      "Name: bowl.n.01_2\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': False, 'InReachOfRobot': True}\n",
      "Name: cabinet.n.01_1\n",
      "{'InFOVOfRobot': False, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "Name: cabinet.n.01_2\n",
      "{'InFOVOfRobot': False, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n",
      "Name: spoon.n.01_1\n",
      "{'InFOVOfRobot': False, 'InSameRoomAsRobot': False, 'InReachOfRobot': True}\n",
      "Name: spoon.n.01_2\n",
      "{'InFOVOfRobot': False, 'InSameRoomAsRobot': False, 'InReachOfRobot': True}\n",
      "Name: piece_of_cloth.n.01_1\n",
      "{'InFOVOfRobot': False, 'InSameRoomAsRobot': False, 'InReachOfRobot': False}\n",
      "Name: sink.n.01_1\n",
      "{'InFOVOfRobot': True, 'InSameRoomAsRobot': True, 'InReachOfRobot': True}\n"
     ]
    }
   ],
   "source": [
    "object_names_list = list(env.task.object_scope.keys())\n",
    "print(\"object_names_list: \", object_names_list)\n",
    "\n",
    "filtered_object_names_list = [name for name in object_names_list \\\n",
    "                              if name.split('.')[0] not in ['agent', 'floor']]\n",
    "print(\"filtered_object_names_list: \", filtered_object_names_list)\n",
    "\n",
    "for name in filtered_object_names_list:\n",
    "    print(f\"Name: {name}\")\n",
    "    obj = env.task.object_scope[name]\n",
    "    print_properties(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c00cc0eb-c8ca-4450-a066-d4f79b20b7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bowl1 = env.task.object_scope['bowl.n.01_1']\n",
    "cabinet1 = env.task.object_scope['cabinet.n.01_1']\n",
    "bowl1.states[object_states.inside.Inside].get_value(cabinet1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17e99af-7a44-4dcd-a000-9f07ce5d214b",
   "metadata": {},
   "source": [
    "## Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296cf71b-5ac0-49a7-a919-f9af2d12d8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Old code containing navitagion to object\n",
    "#for i, action in enumerate(ctrl_gen):\n",
    "#    robot.apply_action(action)\n",
    "#    s.step()\n",
    "#    if i+1 % 100 == 0:\n",
    "#        print(f\"Executing action {action} at step {i}\")\n",
    "#        render_robot_pov(env, env_config, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
